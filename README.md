- üëã Hi, I‚Äôm @GHANNAMnisrine
- üëÄ I‚Äôm interested in statistics and data science
- üå± Take a look on how you can build a hierarchical function on python from scratch

# Hierarchical Clustering from Scratch with Python

Ce projet impl√©mente un algorithme de **clustering hi√©rarchique ascendant (agglom√©ratif)** d√©velopp√© enti√®rement en Python. Il pr√©sente une alternative aux biblioth√®ques de machine learning en reproduisant cette m√©thode de clustering depuis les principes de base, illustrant ainsi une solide compr√©hension des algorithmes de clustering et des structures de donn√©es.

## Description du Projet

Le clustering hi√©rarchique est une technique de machine learning non supervis√©e permettant de regrouper des donn√©es en clusters. Dans cette approche, chaque observation commence comme un cluster unique, et l'algorithme fusionne progressivement les clusters les plus proches jusqu'√† atteindre un certain nombre de clusters souhait√©.

### √âtapes cl√©s de l'impl√©mentation

1. **G√©n√©ration des Donn√©es Synth√©tiques** : Les donn√©es d'entra√Ænement sont g√©n√©r√©es √† l'aide de distributions normales afin de cr√©er des clusters naturels avec diff√©rentes moyennes et variances. Cela permet d'observer le comportement de l'algorithme sur des donn√©es non lin√©aires.

2. **Calcul de la Distance Euclidienne** : Un calcul personnalis√© de la distance euclidienne est utilis√© pour mesurer la proximit√© entre les points. Cette distance sert de crit√®re pour d√©cider quels clusters fusionner √† chaque √©tape.

3. **Initialisation des Clusters** : Chaque point est initialement assign√© √† son propre cluster. L'algorithme fusionne ensuite les clusters deux √† deux selon la distance calcul√©e jusqu‚Äô√† r√©duire le nombre de clusters √† un niveau souhait√©.

4. **Fusion des Clusters les Plus Proches** : Pour chaque it√©ration, l'algorithme identifie les deux clusters les plus proches, puis les fusionne. Cette √©tape est r√©p√©t√©e jusqu‚Äô√† atteindre le nombre de clusters demand√© par l'utilisateur.

5. **Visualisation des Clusters et du Dendrogramme** : Des graphiques en deux dimensions et un dendrogramme illustrent visuellement les clusters form√©s et montrent les distances entre eux. Ces visualisations facilitent l'√©valuation de la s√©paration et de la coh√©sion entre les clusters.

6. **√âvaluation avec le Coefficient de Silhouette** : Le coefficient de silhouette est utilis√© pour √©valuer la qualit√© des clusters. Il quantifie √† quel point les points sont bien regroup√©s dans leurs clusters respectifs et s√©par√©s des autres clusters, offrant ainsi une mesure objective de la qualit√© du clustering.

### Comp√©tences mises en avant

- **Ma√Ætrise des math√©matiques appliqu√©es** : en particulier les calculs de distance et la logique de clustering.
- **D√©veloppement d‚Äôalgorithmes √† partir de z√©ro** : utilisation de Python pour reproduire un algorithme de machine learning sans d√©pendance √† une biblioth√®que d‚Äôapprentissage automatique.
- **Visualisation de donn√©es** : cr√©ation de visualisations interpr√©tables pour faciliter la compr√©hension des clusters form√©s et √©valuer la performance du mod√®le.
- **√âvaluation des mod√®les de clustering** : int√©gration de m√©triques comme le coefficient de silhouette pour quantifier l'efficacit√© de l'algorithme.

### Visualisation des R√©sultats

Les r√©sultats finaux sont pr√©sent√©s sous forme de graphiques qui montrent chaque cluster en couleur distincte ainsi qu'un dendrogramme d√©taillant le processus de fusion des clusters. Cette visualisation fournit une perspective intuitive et analytique de la mani√®re dont les clusters sont form√©s et comment ils se distinguent les uns des autres.

